/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py exp_manager.exp_dir=/root/ODSC-Hackathon-Repository/results exp_manager.explicit_log_dir=/root/ODSC-Hackathon-Repository/results trainer.devices=1 trainer.num_nodes=1 trainer.precision=bf16 trainer.val_check_interval=200 trainer.max_steps=2500 trainer.gradient_clip_val=0.5 model.megatron_amp_O2=True ++model.mcore_gpt=True model.tensor_model_parallel_size=1 model.pipeline_model_parallel_size=1 model.micro_batch_size=1 model.global_batch_size=16 model.optim.sched.name=CosineAnnealing model.optim.sched.warmup_steps=200 model.optim.lr=1e-6 model.optim.weight_decay=0.01 model.optim.betas=[0.9,0.95] model.restore_from_path=/root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo model.data.train_ds.num_workers=2 model.data.train_ds.add_bos=True model.data.validation_ds.num_workers=1 model.data.train_ds.file_names=[/root/ODSC-Hackathon-Repository/data/split/train.jsonl] model.data.train_ds.concat_sampling_probabilities=[1.0] model.data.validation_ds.file_names=[/root/ODSC-Hackathon-Repository/data/split/val.jsonl] model.peft.peft_scheme=lora