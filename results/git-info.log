commit hash: d1bbe66cc1133a4f5c90f959e58a0ef9a861d286
diff --git a/helpers/docbuilder.py b/helpers/docbuilder.py
index 25b2f8d..0ed523c 100644
--- a/helpers/docbuilder.py
+++ b/helpers/docbuilder.py
@@ -16,7 +16,7 @@ import json
 import os
 import re
 import warnings
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional, Tuple
 
 import requests
 from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
@@ -28,20 +28,16 @@ from nemo_curator.download.doc_builder import (
     DocumentIterator,
 )
 
-# Ignore the specific BeautifulSoup warning
 warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)
 
 def download_and_convert_dataset(output_dir: str) -> pd.DataFrame:
     """
-    Downloads the Law Q&A dataset and processes it into a suitable format.
-
-    Returns:
-        A dataframe corresponding to the processed dataset.
+    Downloads the Law Q&A dataset and processes it into a pandas DataFrame.
     """
     DATASET_URL = "https://huggingface.co/datasets/ymoslem/Law-StackExchange/resolve/main/law-stackexchange-questions-answers.json"
     download_dir = os.path.join(output_dir, "raw")
     os.makedirs(download_dir, exist_ok=True)
-    # Download the dataset in raw format and convert it to JSONL.
+
     downloader = LawQADownloader(download_dir)
     raw_fp = downloader.download(DATASET_URL)
 
@@ -96,91 +92,68 @@ class LawQADownloader(DocumentDownloader):
 
 
 class LawQAIterator(DocumentIterator):
-
     def __init__(self):
         super().__init__()
         self._counter = -1
         self._extractor = LawQAExtractor()
 
     def iterate(self, file_path):
-        """
-        Iterates over the content of a file and yields extracted records.
-
-        Args:
-            file_path (str): The path to the file to be iterated.
-
-        Yields:
-            dict: A dictionary representing a record extracted from the file.
-        """
         self._counter = -1
         file_name = os.path.basename(file_path)
 
         with open(file_path, "r", encoding="utf-8") as file:
-            lines = file.readlines()
-
-        file_content = "".join(lines)
-        json_content = json.loads(file_content)
+            json_content = json.load(file)
 
         for row in json_content:
             self._counter += 1
             extracted_content = self._extractor.extract(row)
 
-            # Skip if the question has no answers.
             if extracted_content is None:
                 continue
 
-            id, extracted_content = extracted_content
+            id, content = extracted_content
             meta = {
                 "filename": file_name,
                 "id": f"law-stackexchange-qa-{id}",
             }
 
-            record = {**meta, **extracted_content}
-            yield record
+            yield {**meta, **content}
 
 
 class LawQAExtractor(DocumentExtractor):
-
-    def extract(self, content: str) -> Dict[str, str]:
-        """
-        Extracts relevant information from a law-related question and its best answer.
-
-        Args:
-            content (str): The content of the question and its answers.
-
-        Returns:
-            Dict[str, str]: A dictionary containing the extracted information, including the question ID, title, body,
-            score, best answer, best answer score, and tags.
-        """
-        id = content["question_id"]
-        q_title = content["question_title"]
-        q_body = content["question_body"]
-        q_score = content["score"]
-        tags = ",".join(sorted(content["tags"]))
-
-        # If this question has no answers, skip it.
-        if len(content["answers"]) == 0:
+    def extract(self, content: Dict[str, Any]) -> Optional[Tuple[str, Dict[str, Any]]]:
+        try:
+            if not content.get("answers"):
+                return None
+
+            id = content["question_id"]
+            q_title = content["question_title"]
+            q_body = content["question_body"]
+            q_score = content["score"]
+            tags = ",".join(sorted(content["tags"]))
+
+            best_answer = content["answers"][0]
+            best_answer_score = best_answer["score"]
+            best_answer_text = best_answer["body"]
+
+            # Clean HTML
+            q_title = self._clean_html(q_title)
+            q_body = self._clean_html(q_body)
+            best_answer_text = self._clean_html(best_answer_text)
+
+            return id, {
+                "title": q_title,
+                "question": q_body,
+                "question_score": q_score,
+                "answer": best_answer_text,
+                "answer_score": best_answer_score,
+                "tags": tags
+            }
+        except Exception as e:
+            print(f"Error processing content: {str(e)}")
             return None
 
-        # All answers are sorted by votes, so take the first answer as the best one.
-        best_answer = content["answers"][0]
-        best_answer_score = best_answer["score"]
-        best_answer = best_answer["body"]
-
-        # Get rid of HTML tags using beautifulsoup
-        # NOTE: Doing this here so that I can split the dataset without having to worry about curating the test split.
-        q_title = self._clean_html(q_title)
-        q_body = self._clean_html(q_body)
-        best_answer = self._clean_html(best_answer)
-
-        return id, {
-            "title": q_title,
-            "question": q_body,
-            "question_score": q_score,
-            "answer": best_answer,
-            "answer_score": best_answer_score,
-            "tags": tags,
-        }
+
 
     def _clean_html(self, text: str) -> str:
         text = BeautifulSoup(text, "lxml").get_text()
diff --git a/helpers/modifiers.py b/helpers/modifiers.py
index 6ea8106..754675b 100644
--- a/helpers/modifiers.py
+++ b/helpers/modifiers.py
@@ -14,30 +14,69 @@
 
 import re
 import warnings
+from typing import Any, Dict, List, Optional, Tuple
 
 from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
 
 from nemo_curator.modifiers import DocumentModifier
+from nemo_curator.modifiers.pii_modifier import PiiModifier
+from nemo_curator.datasets import DocumentDataset
+from nemo_curator.modules.modify import Modify
 
-# Ignore the specific BeautifulSoup warning
 warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)
 
 
-class CleanHTML(DocumentModifier):
-    """
-    A simple modifier that removes HTML tags from the document.
-    """
-
-    def modify_document(self, text: str) -> str:
+class SectionNumberFormatter(DocumentModifier):
+    def modify_document(self, document: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Formats section numbers in legal text.
         """
-        Removes HTML tags from the document.
+        if isinstance(document, str):
+            text = document
+        elif isinstance(document, dict) and "question" in document:
+            text = document["question"]
+        else:
+            return document
 
-        Args:
-            text (str): The text to be modified.
+        # Format section numbers
+        text = re.sub(r'§(\S)', r'§ \1', text)
+        text = re.sub(r'§§(\S)', r'§§ \1', text)
+        text = re.sub(r'(\d+[a-z]?)-(\d+)', r'\1–\2', text)
+        text = re.sub(r'(\d+)([a-z]{1,3})\b', r'\1\2', text)
+        text = re.sub(r'(\d+[a-z]{2})-(\d+)', r'\1–\2', text)
+        text = re.sub(r'(§§? \d+)-(\d+)', r'\1–\2', text)
+        text = re.sub(r'([a-z]{2})-(\d+)', r'\1–\2', text)
+        text = re.sub(r'\bUSC\b', 'U.S.C.', text)
+        text = re.sub(r'§(\d)', r'§ \1', text)
+        text = re.sub(r'§§(\d)', r'§§ \1', text)
+        text = re.sub(r'(§§? \d+)-(\d+)', r'\1–\2', text)
+        text = re.sub(r'\(([a-z])\)', r'(\1)', text)
+        text = re.sub(r'\(([a-z])\)-\(([a-z])\)', r'(\1)–(\2)', text)
+        text = re.sub(r'(\d+)\s+U\.S\.C\.', r'\1 U.S.C.', text)
+        text = re.sub(r'(\d+)\s+U\.S\.C\.\s+§\s+(\d+)(\([a-z]\)(?:-\([a-z]\))?)?', 
+                      lambda m: f"{m.group(1)} U.S.C. § {m.group(2)}{m.group(3) if m.group(3) else ''}", 
+                      text)
 
-        Returns:
-            str: The modified text.
-        """
-        text = BeautifulSoup(text, "lxml").get_text()
-        # Remove extra whitespaces and newlines.
-        return re.sub(r"\s+", " ", text).strip()
+        if isinstance(document, str):
+            return text
+        document["question"] = text
+        return document
+
+
+def redact_pii(dataset : DocumentDataset, text_field):
+    pii_redactor = Modify(
+        PiiModifier(
+            supported_entities=[
+                "ADDRESS",
+                "EMAIL_ADDRESS",
+                "LOCATION",
+                "PERSON",
+                "URL",
+                "PHONE_NUMBER",
+            ],
+            anonymize_action="replace",
+            device="cpu",
+        ),
+        text_field = text_field,
+    )
+    return pii_redactor(dataset)
